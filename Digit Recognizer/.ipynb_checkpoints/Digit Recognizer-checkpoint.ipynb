{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d6981b5",
   "metadata": {},
   "source": [
    "# Digit Recognizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff32d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import keras\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# Display the number of rows and columns in the dataset\n",
    "print(\"Dataset shape:\", train_data.shape)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"\\nFirst few rows of the dataset:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Check the distribution of the target variable (digits)\n",
    "digit_counts = train_data['label'].value_counts().sort_index()\n",
    "print(\"\\nDistribution of digits:\")\n",
    "print(digit_counts)\n",
    "\n",
    "# Visualize a few sample images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    image = train_data.iloc[i, 1:].values.reshape(28, 28)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.set_title(\"Digit: {}\".format(train_data.iloc[i, 0]))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cde4d0",
   "metadata": {},
   "source": [
    "### Data Preprocessing Insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2138c73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = train_data.isnull().sum()\n",
    "print(\"Missing values:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd76490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the class distribution\n",
    "class_distribution = train_data['label'].value_counts()\n",
    "print(\"Class distribution:\\n\", class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2828f1",
   "metadata": {},
   "source": [
    "### Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd7600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_data.drop(\"label\", axis=1)\n",
    "y_train = train_data[\"label\"]\n",
    "\n",
    "# Scale the pixel values between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Reshape the training and test data\n",
    "X_train_scaled = X_train_scaled.reshape(-1, 28, 28, 1)\n",
    "X_test_scaled = X_test_scaled.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17452737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data into a training set and a validation set\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a112e017",
   "metadata": {},
   "source": [
    "###    CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51e68a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CNN model\n",
    "model = Sequential([\n",
    "  Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D((2, 2)),\n",
    "  Conv2D(64, (3, 3), activation='relu'),\n",
    "  MaxPooling2D((2, 2)),\n",
    "  Conv2D(128, (3, 3), activation='relu'),\n",
    "  MaxPooling2D((2, 2)),\n",
    "  Flatten(),\n",
    "  Dense(128, activation='relu'),\n",
    "  Dropout(0.5),\n",
    "  Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=64, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e0a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a CNN model\n",
    "# model = Sequential([\n",
    "#   Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "#   MaxPooling2D((2, 2)),\n",
    "#   Conv2D(64, (3, 3), activation='relu'),\n",
    "#   MaxPooling2D((2, 2)),\n",
    "#   Conv2D(128, (3, 3), activation='relu'),\n",
    "#   MaxPooling2D((2, 2)),\n",
    "#   Flatten(),\n",
    "#   Dense(128, activation='relu'),\n",
    "#   Dropout(0.5),\n",
    "#   Dense(10, activation='softmax')\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam',\n",
    "#               loss='sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # Train the model\n",
    "# model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a01470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    \"ImageId\": test_data.index + 1,  # Adding 1 to match the Kaggle submission index\n",
    "    \"Label\": predictions.argmax(axis=1)\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97cc49",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "In this Jupyter notebook, I have built a CNN model to classify handwritten digits. We started by loading the training data and performing some basic data exploration. We then scaled the pixel values in the training data and reshaped it to a format that is compatible with our CNN model. I also created a validation set to evaluate the performance of our model.\n",
    "\n",
    "Next, I built our CNN model. My model consists of four convolutional layers, each followed by a max pooling layer. I then flattened the output of the last pooling layer and connected it to a dense layer with 10 output neurons, one for each digit, and used the adam optimizer and the sparse_categorical_crossentropy loss function to train our model.\n",
    "\n",
    "I trained the model for 100 epochs and evaluated its performance on the validation set after each epoch. I saw that the model's accuracy on the validation set steadily increased over time. After 100 epochs, the model achieved an accuracy of 0.9880% on the validation set.\n",
    "\n",
    "Finally, I used our model to make predictions on the test set. I saved the predictions to a CSV file, which then I can submit to the Kaggle competition.\n",
    "\n",
    "The steps I followed in this notebook can be summarized as follows:\n",
    "\n",
    "* Load the training data.\n",
    "* Perform basic data exploration.\n",
    "* Scale the pixel values in the training data.\n",
    "* Reshape the training data.\n",
    "* Create a validation set.\n",
    "* Build a CNN model.\n",
    "* Train the CNN model.\n",
    "* Evaluate the CNN model on the validation set.\n",
    "* Make predictions on the test set.\n",
    "* Save the predictions to a CSV file.\n",
    "\n",
    "Lastly, My submission scored 0.98485% in the Kaggle competition.\n",
    "\n",
    "Emmett(Umut) Demirhan\n",
    "\n",
    "https://www.kaggle.com/emmettdemirhan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
